{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import hydra\n",
    "\n",
    "from cliport.dataset import RavensDataset\n",
    "from cliport.utils import utils\n",
    "from cliport import tasks\n",
    "from cliport.environments.environment import Environment\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### task settings\n",
    "mode = 'test'\n",
    "augment = True\n",
    "\n",
    "### Uncomment the task you want to generate ###\n",
    "#task = 'stack-block-pyramid-seq-seen-colors'\n",
    "#task = 'packing-shapes-train'\n",
    "task = 'packing-and-stacking'\n",
    "\n",
    "### visualization settings\n",
    "max_episodes = 1\n",
    "max_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs\n",
    "root_dir = '/home/jerrylin/temp/cliport'\n",
    "config_file = 'train.yaml' \n",
    "cfg = utils.load_hydra_config(os.path.join(root_dir, f'cliport/cfg/{config_file}'))\n",
    "\n",
    "# Override defaults\n",
    "cfg['task'] = task\n",
    "cfg['mode'] = mode\n",
    "\n",
    "data_dir = os.path.join(root_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks.names[cfg['task']]()\n",
    "task.mode = mode\n",
    "\n",
    "ds = RavensDataset(os.path.join(data_dir, f'{cfg[\"task\"]}-{cfg[\"mode\"]}'), cfg, augment=augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m episode_descrptions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(max_steps, \u001b[38;5;28mlen\u001b[39m(episode))):\n\u001b[0;32m---> 15\u001b[0m     obs_init, obs_goal, embedding \u001b[38;5;241m=\u001b[39m episode[step]\n\u001b[1;32m     16\u001b[0m     num_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m     cam_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2304x2304 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_images = 0\n",
    "\n",
    "episode, seed = ds.load(0)\n",
    "\n",
    "total_images += len(episode)-1\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "fig = plt.figure(figsize=(32, 32))\n",
    "columns = len(episode)\n",
    "rows = 1\n",
    "episode_images = []\n",
    "episode_descrptions = []\n",
    "for step in range(min(max_steps, len(episode))):\n",
    "    obs_init, obs_goal, embedding = episode[step]\n",
    "    num_images = len(obs['color'])\n",
    "    \n",
    "    cam_config = None\n",
    "    if b'camera_info' in info:\n",
    "        cam_config = ds.get_cam_config(info[b'camera_info'])\n",
    "    \n",
    "    img_depth = ds.get_image(obs, cam_config=cam_config)\n",
    "    img_tensor = torch.from_numpy(img_depth)\n",
    "    img = np.uint8(img_tensor.detach().cpu().numpy())\n",
    "    img = img.transpose(1,0,2)\n",
    "    \n",
    "    if step < len(episode)-1 and episode[step]:\n",
    "        batch = ds.process_sample(episode[step], augment=augment)\n",
    "    else:\n",
    "        batch = ds.process_goal(episode[step], perturb_params=None)\n",
    "\n",
    "    img_sample = batch['img']\n",
    "    img_sample = torch.from_numpy(img_sample)\n",
    "    color = np.uint8(img_sample.detach().cpu().numpy())[:,:,:3]\n",
    "    color = color.transpose(1,0,2)\n",
    "    episode_images.append(color)\n",
    "    episode_descrptions.append(batch['lang_goal'])\n",
    "    fig.add_subplot(rows, columns, step + 1)\n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    plt.imshow(color)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_action_description_to_pre_and_post_conditions(text):\n",
    "    text_tokens = text.split()\n",
    "    text_tokens = text_tokens[1:]\n",
    "    text_tokens = ' '.join(text_tokens)\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "for i in range(len(episode_descrptions)-1):\n",
    "    processed_text.append(convert_action_description_to_pre_and_post_conditions(episode_descrptions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "processed_images = []\n",
    "for i in range(len(episode_images)):\n",
    "    image = preprocess(Image.fromarray(np.uint8(episode_images[i]))).unsqueeze(0).to(device)\n",
    "    processed_images.append(image)\n",
    "processed_images = torch.cat(processed_images)\n",
    "text = clip.tokenize(processed_text).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_per_image, logits_per_text = model(processed_images, text)\n",
    "    probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "processed_images = []\n",
    "for i in range(len(episode_images)):\n",
    "    image = preprocess(Image.fromarray(np.uint8(episode_images[i]))).unsqueeze(0).to(device)\n",
    "    processed_images.append(image)\n",
    "processed_images = torch.cat(processed_images)\n",
    "text = clip.tokenize(episode_descrptions).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_per_image, logits_per_text = model(processed_images, text)\n",
    "    probs = logits_per_text.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_descrptions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "683f778b2522716e1799a13edb03530bfa61996d4d2114a91a57cafb85fad712"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
